# -*- coding: utf-8 -*-
"""Data_Extraction_and_Text_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhuEKFjWjAoo-64YD2-HRi7Q7qnjnCD9
"""

# Import necessary libraries
import os                    # Operating system-related functions
import pandas as pd          # Data manipulation and analysis library
import requests              # HTTP library for making web requests
from bs4 import BeautifulSoup  # Library for parsing HTML and XML documents
import numpy as np           # Numerical computing library
from google.colab import files  # Library for interacting with files in Google Colab
import nltk                  # Natural Language Toolkit for text processing
from nltk.tokenize import word_tokenize  # Tokenization function from NLTK
from nltk.corpus import stopwords      # Stopwords from NLTK for text preprocessing
nltk.download('punkt')       # Download NLTK's punkt tokenizer data (punctuations)
nltk.download('stopwords')   # Download NLTK's stopwords data
import re                    # Regular expressions library for text processing

# Import the 'drive' module from 'google.colab' library
from google.colab import drive

# Mount Google Drive to the '/content/drive' directory in Colab
drive.mount('/content/drive')

df=pd.read_excel('/content/drive/MyDrive/Assignment/Input.xlsx')[['URL_ID','URL']]

# Select and create a new DataFrame containing the first 150 rows of the original 'df'.
# The original 'df' remains unchanged.
df = df.iloc[0:150]

df

df.drop('URL_ID',axis=1,inplace=True)

# Read data from an Excel file (assuming it contains a 'URL' column and a 'URL_ID' column)
df = pd.read_excel('/content/drive/MyDrive/Assignment/Input.xlsx')

# Loop through each row in the DataFrame
for index, row in df.iterrows():
    # Extract URL and URL_ID from the current row
    url = row['URL']
    url_id = row['URL_ID']

    # Make an HTTP request to the URL with a user-agent header
    header = {
        'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=header)
    except:
        # Handle exceptions if there's an issue with the request
        print("Can't get response of {}".format(url_id))

    # Create a BeautifulSoup object to parse the HTML content of the page
    try:
        soup = BeautifulSoup(response.content, 'html.parser')
    except:
        # Handle exceptions if there's an issue with parsing the HTML
        print("Can't get page of {}".format(url_id))

    # Find the title of the webpage (assuming it's wrapped in an 'h1' tag)
    try:
        title = soup.find('h1').get_text()
    except:
        # Handle exceptions if there's an issue with finding the title
        print("Can't get title of {}".format(url_id))
        continue  # Skip this URL and move to the next one

    # Initialize a variable to store the article text
    article = ""
    try:
        # Find all 'p' tags and concatenate their text into the 'article' variable
        for p in soup.find_all('p'):
            article += p.get_text()
    except:
        # Handle exceptions if there's an issue with extracting the text
        print("Can't get text of {}".format(url_id))

    # Define the file name where the title and article text will be saved
    file_name = '/content/drive/MyDrive/Assignment/TitleText/' + str(url_id) + '.txt'

    # Write the title and article text to the file
    with open(file_name, 'w') as file:
        file.write(title + '\n' + article)

# Directories where files are located
text_dir = "/content/drive/MyDrive/Assignment/TitleText"  # Directory for text files
stopwords_dir = "/content/drive/MyDrive/Assignment/StopWords"  # Directory for stop words
sentiment_dir = "/content/drive/MyDrive/Assignment/MasterDictionary"  # Directory for sentiment analysis files

# Initialize a set to store stop words
stop_words = set()

# Load stop words from the 'stopwords_dir' directory and add them to the 'stop_words' set
for files in os.listdir(stopwords_dir):
    with open(os.path.join(stopwords_dir, files), 'r', encoding='ISO-8859-1') as f:
        stop_words.update(set(f.read().splitlines()))

# Initialize a list to store text from files
docs = []

# Load text files from the 'text_dir' directory and process them
for text_file in os.listdir(text_dir):
    with open(os.path.join(text_dir, text_file), 'r') as f:
        text = f.read()
        # Tokenize the text
        words = word_tokenize(text)
        # Remove stop words from the tokens
        filtered_text = [word for word in words if word.lower() not in stop_words]
        # Add filtered tokens to the 'docs' list
        docs.append(filtered_text)

# Initialize sets to store positive and negative words
pos = set()
neg = set()

# Load positive and negative words from the 'sentiment_dir' directory
for files in os.listdir(sentiment_dir):
    if files == 'positive-words.txt':
        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:
            pos.update(f.read().splitlines())
    else:
        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:
            neg.update(f.read().splitlines())

# Initialize lists to store scores
positive_words = []
Negative_words = []
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []

# Iterate through the list of documents (tokenized text)
# Iterate through the list of documents (tokenized text)
for i in range(len(docs)):
    # Find words in the current document that are in the 'pos' (positive) set and store them in 'positive_words'
    positive_words.append([word for word in docs[i] if word.lower() in pos])

    # Find words in the current document that are in the 'neg' (negative) set and store them in 'Negative_words'
    Negative_words.append([word for word in docs[i] if word.lower() in neg])

    # Calculate the count of positive words in the current document and store it in 'positive_score'
    positive_score.append(len(positive_words[i]))

    # Calculate the count of negative words in the current document and store it in 'negative_score'
    negative_score.append(len(Negative_words[i]))

    # Calculate the polarity score for the current document using the formula
    # (positive_score - negative_score) / (positive_score + negative_score + 0.000001)
    # and store it in 'polarity_score'
    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))

    # Calculate the subjectivity score for the current document using the formula
    # (positive_score + negative_score) / (len(docs[i]) + 0.000001)
    # and store it in 'subjectivity_score'
    subjectivity_score.append((positive_score[i] + negative_score[i]) / (len(docs[i]) + 0.000001))

import os
import re
import nltk

nltk.download('stopwords')  # This downloads the stopwords data

from nltk.corpus import stopwords

# Average Sentence Length = the number of words / the number of sentences
# Percentage of Complex words = the number of complex words / the number of words
# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)

avg_sentence_length = []
Percentage_of_Complex_words  =  []
Fog_Index = []
complex_word_count =  []
avg_syllable_word_count =[]

stopwords = set(stopwords.words('english'))
def measure(file):
  with open(os.path.join(text_dir, file),'r') as f:
    text = f.read()
# remove punctuations
    text = re.sub(r'[^\w\s.]','',text)
# split the given text file into sentences
    sentences = text.split('.')
# total number of sentences in a file
    num_sentences = len(sentences)
# total words in the file
    words = [word  for word in text.split() if word.lower() not in stopwords ]
    num_words = len(words)

# complex words having syllable count is greater than 2
# Complex words are words in the text that contain more than two syllables.
    complex_words = []
    for word in words:
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word > 2:
        complex_words.append(word)

# Syllable Count Per Word
# We count the number of Syllables in each word of the text by counting the vowels present in each word.
#  We also handle some exceptions like words ending with "es","ed" by not counting them as a syllable.
# A syllable is a unit of sound in a word, typically consisting of a vowel sound and any accompanying consonant sounds
    syllable_count = 0
    syllable_words =[]
    for word in words:
      if word.endswith('es'):
        word = word[:-2]
      elif word.endswith('ed'):
        word = word[:-2]
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word >= 1:
        syllable_words.append(word)
        syllable_count += syllable_count_word


    avg_sentence_len = num_words / num_sentences
    avg_syllable_word_count = syllable_count / len(syllable_words)
    Percent_Complex_words  =  len(complex_words) / num_words
    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)

    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count

# iterate through each file or doc
for file in os.listdir(text_dir):
  x,y,z,a,b = measure(file)
  avg_sentence_length.append(x)
  Percentage_of_Complex_words.append(y)
  Fog_Index.append(z)
  complex_word_count.append(a)
  avg_syllable_word_count.append(b)

# Function to calculate word count and average word length for a text file
def cleaned_words(file):
    with open(os.path.join(text_dir, file), 'r') as f:
        text = f.read()

        # Remove punctuations from the text
        text = re.sub(r'[^\w\s]', '', text)

        # Split the text into words and remove stopwords
        words = [word for word in text.split() if word.lower() not in stopwords]

        # Calculate the total number of characters in all words
        length = sum(len(word) for word in words)

        # Calculate the average word length
        average_word_length = length / len(words)

    return len(words), average_word_length

# Lists to store word count and average word length
word_count = []
average_word_length = []

# Iterate through each file or document in the directory
for file in os.listdir(text_dir):
    x, y = cleaned_words(file)
    word_count.append(x)
    average_word_length.append(y)

# Function to count personal pronouns in a text file
def count_personal_pronouns(file):
    with open(os.path.join(text_dir, file), 'r') as f:
        text = f.read()
        personal_pronouns = ["I", "we", "my", "ours", "us"]
        count = 0
        for pronoun in personal_pronouns:
            # Using regex to find occurrences of the personal pronoun with word boundaries (\b)
            count += len(re.findall(r"\b" + pronoun + r"\b", text))
    return count

# List to store the counts of personal pronouns for each file
pp_count = []

# Iterate through each file or document in the directory
for file in os.listdir(text_dir):
    x = count_personal_pronouns(file)
    pp_count.append(x)

# Function to calculate the average number of words per sentence in a text file
def calculate_avg_no_of_words_per_sentence(file):
    with open(file, 'r') as f:
        text = f.read()
    sentences = text.split('.')  # Split the text into sentences
    num_sentences = len(sentences)
    words = [word for word in text.split() if word.lower() not in stopwords]
    num_words = len(words)
    avg_words_per_sentence = num_words / num_sentences
    return avg_words_per_sentence

# List to store the average number of words per sentence for each file
avg_words_per_sentence_list = []

# Iterate through each file or document in the directory
for file in os.listdir(text_dir):
    file_path = os.path.join(text_dir, file)
    avg_words_per_sentence = calculate_avg_no_of_words_per_sentence(file_path)
    avg_words_per_sentence_list.append(avg_words_per_sentence)

# Calculate the overall average number of words per sentence for all files
average_words_per_sentence = sum(avg_words_per_sentence_list) / len(avg_words_per_sentence_list)
print("Average number of words per sentence:", average_words_per_sentence)

# Read data from an Excel file into a DataFrame
output_df = pd.read_excel('/content/drive/MyDrive/Assignment/Output Data Structure.xlsx')

# URL_ID 44, 57, and 144 do not exist (i.e., the pages do not exist and throw 404 errors),
# so we are going to drop these rows from the DataFrame
output_df.drop([44-37, 57-37, 144-37], axis=0, inplace=True)

# Define the required parameters in a list
variables = [positive_score,
             negative_score,
             polarity_score,
             subjectivity_score,
             avg_sentence_length,
             Percentage_of_Complex_words,
             Fog_Index,
             avg_words_per_sentence_list,
             complex_word_count,
             word_count,
             avg_syllable_word_count,
             pp_count,
             average_word_length]

# Write the values to the DataFrame
for i, var in enumerate(variables):
    output_df.iloc[:, i+2] = var

# Save the modified DataFrame to a CSV file
output_df.to_csv('/content/drive/MyDrive/Assignment/Output.xlsx')